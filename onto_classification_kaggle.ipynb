{
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SOi6b7j8RerD",
        "8KwDYtolSlOw",
        "2q5ey7tSTDgL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library Import"
      ],
      "metadata": {
        "id": "SOi6b7j8RerD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- BASE IMPORT -----\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv,nltk\n",
        "from numpy import array\n",
        "import string\n",
        "import random\n",
        "from math import floor\n",
        "from collections import OrderedDict\n",
        "from collections import defaultdict\n",
        "\n",
        "# ----- NATURAL LANGUAGE IMPORT -----\n",
        "!python -m nltk.downloader all\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import  pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from operator import itemgetter, attrgetter\n",
        "\n",
        "# ----- MACHINE LEARNING IMPORT -----\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xrFMehRReYi",
        "outputId": "6af62957-07a0-453c-b08e-acef902b4294",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:37:07.933064Z",
          "iopub.execute_input": "2022-09-21T11:37:07.933569Z",
          "iopub.status.idle": "2022-09-21T11:37:36.270796Z",
          "shell.execute_reply.started": "2022-09-21T11:37:07.933454Z",
          "shell.execute_reply": "2022-09-21T11:37:36.269238Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/opt/conda/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/dolch.zip.\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package extended_omw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/pe08.zip.\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/perluniprops.zip.\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions Definition"
      ],
      "metadata": {
        "id": "ezHTfjhOSg_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Managment Functions"
      ],
      "metadata": {
        "id": "8KwDYtolSlOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_creation():\n",
        "    disease_symptom_dict = defaultdict(list)\n",
        "    with open('../input/ontobasedclassification/Final_Dataset.csv','r',encoding='utf-8') as csvFile:\n",
        "      reader=csv.reader(csvFile)\n",
        "      data=list(reader)\n",
        "    csvFile.close()\n",
        "    keywords=[]\n",
        "    # print(len(data))\n",
        "    for i in range(1,len(data)):\n",
        "      # print(i,'Entity:'+data[i][0])\n",
        "      data[i][0].replace('\\xa0',' ')\n",
        "      # print('Keywords:'+data[i][1])\n",
        "      # print('Additional Keywords:'+data[i][2])\n",
        "      for j in range(0,len(data[i][1])):\n",
        "        data[i][1][j].replace('\\xa0',' ')\n",
        "      keywords=data[i][1].split(',')\n",
        "      # print(keywords)\n",
        "      disease_symptom_dict[str(data[i][0]).strip()].append(keywords)\n",
        "    # print(len(disease_symptom_dict))\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in disease_symptom_dict:\n",
        "        y.append(i)\n",
        "        x.append(disease_symptom_dict[i][0])\n",
        "    temp = []\n",
        "    for i in x:\n",
        "        for j in i:\n",
        "            temp.append(j)\n",
        "\n",
        "    from collections import OrderedDict\n",
        "    temp = list(OrderedDict((x, True) for x in temp).keys())\n",
        "    for i in range(len(y)):\n",
        "      for j in range(150):\n",
        "          y.append(y[i])\n",
        "          x.append(random.sample(x[i],floor(len(x[i])/3)))\n",
        "    x_ = []\n",
        "    for i in x:\n",
        "      x_.append(' '.join(i))\n",
        "    return x_,y\n",
        "\n",
        "    # with this function we have symptom and the associated disease into a dataframe"
      ],
      "metadata": {
        "id": "6pici8FARbRI",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:39:02.483050Z",
          "iopub.execute_input": "2022-09-21T11:39:02.483521Z",
          "iopub.status.idle": "2022-09-21T11:39:02.495799Z",
          "shell.execute_reply.started": "2022-09-21T11:39:02.483484Z",
          "shell.execute_reply": "2022-09-21T11:39:02.494562Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language Functions and Classes"
      ],
      "metadata": {
        "id": "2q5ey7tSTDgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cleaning_nlp_processes:\n",
        "\n",
        "    def __init__(self,description):\n",
        "        self.description=description\n",
        "\n",
        "    def same_frequency_words_priority(self,word_list):\n",
        "        final_list=[]\n",
        "        parts_of_speech = self.parts_of_speech()\n",
        "        for i in range(0,len(word_list)):\n",
        "            words = self.lemmatization(parts_of_speech, [word_list[i][0]])\n",
        "            if len(words)==0 or word_list[i][0]==words[0]:\n",
        "                # print(words,word_list[i],\"Flag=1\")\n",
        "                final_list.append((word_list[i][0],word_list[i][1],1))\n",
        "            else:\n",
        "                # print(words,word_list[i],\" Flag=0\")\n",
        "                final_list.append((word_list[i][0],word_list[i][1],0))\n",
        "        return final_list\n",
        "\n",
        "\n",
        "    def priority_provide(self, stop_words_remove,freq_dict):\n",
        "        final_priority_list=[]\n",
        "        # print(freq_dict)\n",
        "        import operator\n",
        "        sorted_x = sorted(freq_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
        "        import collections\n",
        "        sorted_dict = dict(collections.OrderedDict(sorted_x))\n",
        "        sorted_list = [(k, v) for k, v in sorted_dict.items()]\n",
        "        # print(sorted_list)\n",
        "        splice_index=[]\n",
        "        for i in range(1,len(sorted_list)):\n",
        "            if sorted_list[i-1][1]==sorted_list[i][1]:\n",
        "                continue\n",
        "            else:\n",
        "                splice_index.append(i)\n",
        "        # print(splice_index)\n",
        "        if len(splice_index)==0:\n",
        "            final_priority_list=self.same_frequency_words_priority(sorted_list)\n",
        "\n",
        "        elif len(splice_index)==1:\n",
        "            list_1=(sorted_list[:splice_index[0]])\n",
        "            list_2=(sorted_list[splice_index[0]:])\n",
        "            final_priority_list = self.same_frequency_words_priority(list_1)\n",
        "            final_priority_list += self.same_frequency_words_priority(list_2)\n",
        "\n",
        "        else:\n",
        "            list_1=(sorted_list[:splice_index[0]])\n",
        "            final_priority_list=self.same_frequency_words_priority(list_1)\n",
        "\n",
        "            for i in range(1,len(splice_index)):\n",
        "                list_i=(sorted_list[splice_index[i-1]:splice_index[i]])\n",
        "                final_priority_list+=self.same_frequency_words_priority(list_i)\n",
        "            list_end=(sorted_list[splice_index[len(splice_index)-1]:])\n",
        "            final_priority_list += self.same_frequency_words_priority(list_end)\n",
        "\n",
        "        # print(final_priority_list)\n",
        "        sorted_x = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "        return final_priority_list\n",
        "\n",
        "    def keywords_formation(self):\n",
        "        word_arr = self.tokenizing([self.description])\n",
        "        stopwords_remove = self.stop_words_removal(word_arr)\n",
        "        # print(stopwords_remove)\n",
        "        count = self.freq(stopwords_remove)\n",
        "        final_prority_array=self.priority_provide(stopwords_remove,count)\n",
        "        # print(final_prority_array)\n",
        "        return final_prority_array\n",
        "\n",
        "    def tokenizing(self, parameters):\n",
        "        # Tokenising..............................................\n",
        "        data = '';\n",
        "        for i in parameters:\n",
        "            data += i + ' '\n",
        "        data = data.strip()\n",
        "        # print(data)\n",
        "        data = data.split(' ')\n",
        "        # print(data)\n",
        "        sentence = ''\n",
        "        for i in data:\n",
        "            sentence += i+' '\n",
        "        sentence=sentence.strip()\n",
        "        word_arr = word_tokenize(sentence.lower())\n",
        "        #  Here lower case is done to remove more stopwords, but some information is lost\n",
        "        # print( word_arr)\n",
        "        return word_arr\n",
        "\n",
        "    def stop_words_removal(self, word_arr):\n",
        "        # Stopwords and Punctuations.........................................\n",
        "        stop_words = stopwords.words('english')\n",
        "        # Stop Words are present of different languages, for papers of different languages.\n",
        "        # Cleaning words(removing stopwords and punctuations\n",
        "        # print(stop_words)\n",
        "        punctuations = list(string.punctuation)\n",
        "        # print(string.punctuation)\n",
        "        stop_words += punctuations\n",
        "        # print(len(stop_words))\n",
        "        file_data = []\n",
        "        with open('../input/ontobasedclassification/stopwords-1', 'r') as f:\n",
        "            for line in f:\n",
        "                for word in line.split():\n",
        "                    file_data.append(word)\n",
        "        stop_words += file_data\n",
        "        stop_words = set(stop_words)\n",
        "        stop_words = list(stop_words)\n",
        "        new_word_arr = []\n",
        "        for i in word_arr:\n",
        "            new_word_arr.append(i.lower())\n",
        "        clean_words = [w for w in new_word_arr if not w in stop_words]\n",
        "        # print(len(stop_words))\n",
        "        # print(\"After cleaning the stopwords, no of words:\", len(clean_words))\n",
        "        # print(clean_words)\n",
        "        return clean_words\n",
        "\n",
        "    def freq(self, words):\n",
        "        dict = {\n",
        "        }\n",
        "        for i in words:\n",
        "            dict[i] = words.count(i)\n",
        "        # print(\"Dictionary after stopwords removal(no lemmatization)\")\n",
        "        # print(dict)\n",
        "        return dict\n",
        "\n",
        "    def lemmatization(self, part_of_speech, clean_words):\n",
        "        lemmatized = []\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        # print pos[2][0]\n",
        "        # print clean_words[1]\n",
        "        for i in range(0, len(part_of_speech)):\n",
        "            for j in range(0, len(clean_words)):\n",
        "                if part_of_speech[i][0].lower() == clean_words[j]:\n",
        "                    # print part_of_speech[i][0], clean_words[j], part_of_speech[i][1]\n",
        "                    lemmatized.append(\n",
        "                        (lemmatizer.lemmatize(clean_words[j], pos=self.pos_to_wordnet(part_of_speech[i][1]))).lower())\n",
        "                    break\n",
        "                else:\n",
        "                    continue\n",
        "        # print(\"After Lemmatization No. of Words:\", len(list(set(lemmatized))))\n",
        "        # print(lemmatized)\n",
        "        return list(set(lemmatized))\n",
        "\n",
        "    def parts_of_speech(self):\n",
        "        parts_of_speech = pos_tag(word_tokenize(self.description))  # +' '+self.synonym+ ' ' + self.name))\n",
        "        # print(parts_of_speech)\n",
        "        return parts_of_speech\n",
        "\n",
        "    def pos_to_wordnet(self, pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        # elif pos_tag.startswith('M'):\n",
        "        #     return wordnet.MODAL\n",
        "        # elif pos_tag.startswith('R'):\n",
        "        #     return wordnet.ADVERB\n",
        "        else:\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    def stemming(self, cleaning_words):\n",
        "        '''\n",
        "        The process of stemmimng is very dumb. Not always give reslutant output. The information passed through it might result in bad output.\n",
        "        Stemming is the process of finding the root word of the given word.\n",
        "        Prefer Lemmatization over it.\n",
        "        '''\n",
        "        ps = PorterStemmer()\n",
        "        # stem_words = ['play', 'playing', 'played', 'player', \"happy\", 'happier']\n",
        "        stemmed_words = [ps.stem(w) for w in cleaning_words]\n",
        "        # print(len(stemmed_words))\n",
        "        # print(stemmed_words)\n",
        "        return stemmed_words"
      ],
      "metadata": {
        "id": "j9OTExYfTIcI",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:39:14.884675Z",
          "iopub.execute_input": "2022-09-21T11:39:14.885079Z",
          "iopub.status.idle": "2022-09-21T11:39:14.913230Z",
          "shell.execute_reply.started": "2022-09-21T11:39:14.885045Z",
          "shell.execute_reply": "2022-09-21T11:39:14.912142Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the function for the text processing on the dataset"
      ],
      "metadata": {
        "id": "tTl7EedZVBVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('../input/ontobasedclassification/Final_Dataset.csv', 'r') as csvFile:\n",
        "        reader = csv.reader(csvFile)\n",
        "        additional_keywords = list(reader)\n",
        "csvFile.close()\n",
        "    \n",
        "temp=cleaning_nlp_processes(additional_keywords[4][2]+additional_keywords[4][3])\n",
        "temp.keywords_formation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag3zxWj0U8AP",
        "outputId": "9601fb51-6949-418c-cb95-900b1b559b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": [
              "[('internal', 2, 1),\n",
              " ('body', 2, 1),\n",
              " ('organs', 2, 0),\n",
              " ('bowel', 2, 1),\n",
              " ('problems', 2, 0),\n",
              " ('adhesion', 1, 1),\n",
              " ('band', 1, 1),\n",
              " ('scar', 1, 1),\n",
              " ('tissue', 1, 1),\n",
              " ('joins', 1, 0),\n",
              " ('surfaces', 1, 0),\n",
              " ('connected', 1, 0),\n",
              " ('tissues', 1, 0),\n",
              " ('stick', 1, 1),\n",
              " ('adhere', 1, 1),\n",
              " ('surfaces.adhesions', 1, 1),\n",
              " ('affect', 1, 1),\n",
              " ('female', 1, 1),\n",
              " ('reproductive', 1, 1),\n",
              " ('ovaries', 1, 0),\n",
              " ('fallopian', 1, 1),\n",
              " ('tubes', 1, 0),\n",
              " ('area', 1, 1),\n",
              " ('heart', 1, 1),\n",
              " ('spine', 1, 1),\n",
              " ('hand', 1, 1),\n",
              " ('cause', 1, 1),\n",
              " ('range', 1, 1),\n",
              " ('including', 1, 0),\n",
              " ('infertility', 1, 1),\n",
              " ('dyspareunia', 1, 1),\n",
              " ('painful', 1, 1),\n",
              " ('intercourse', 1, 1),\n",
              " ('pelvic', 1, 1),\n",
              " ('pain', 1, 1),\n",
              " ('obstruction', 1, 1),\n",
              " ('blockage', 1, 1),\n",
              " ('adhesions', 1, 0),\n",
              " ('lead', 1, 1),\n",
              " ('complex', 1, 1),\n",
              " ('set', 1, 1),\n",
              " ('called', 1, 0),\n",
              " ('adhesion-related', 1, 1),\n",
              " ('disorder', 1, 1),\n",
              " ('ard', 1, 1)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ontology Managment Functions"
      ],
      "metadata": {
        "id": "WBj7inQqU3vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class keywords_from_ontology:\n",
        "    def __init__(self,properties):\n",
        "        # self.defintion=definition\n",
        "        # self.synonym=synonym\n",
        "        # self.entity_name=entity_name\n",
        "        self.properties=properties\n",
        "\n",
        "    def keywords_creation(self):\n",
        "        tokens=self.tokenizing(self.properties)\n",
        "        clean_words=self.stop_words_removal(tokens)\n",
        "        return clean_words\n",
        "\n",
        "    def stop_words_removal(self, word_arr):\n",
        "        # Stopwords and Punctuations.........................................\n",
        "        stop_words = stopwords.words('english')\n",
        "        # Stop Words are present of different languages, for papers of different languages.\n",
        "        # Cleaning words(removing stopwords and punctuations\n",
        "        # print(stop_words)\n",
        "        \n",
        "        punctuations = list(string.punctuation)\n",
        "        # print(string.punctuation)\n",
        "        stop_words += punctuations\n",
        "        # print(len(stop_words))\n",
        "        file_data = []\n",
        "        with open('../input/ontobasedclassification/stopwords-1', 'r') as f:\n",
        "            for line in f:\n",
        "                for word in line.split():\n",
        "                    file_data.append(word)\n",
        "        stop_words += file_data\n",
        "        stop_words = set(stop_words)\n",
        "        stop_words = list(stop_words)\n",
        "        new_word_arr = []\n",
        "        for i in word_arr:\n",
        "            new_word_arr.append(i.lower())\n",
        "        clean_words = [w for w in new_word_arr if not w in stop_words]\n",
        "        # print(len(stop_words))\n",
        "        # print(\"After cleaning the stopwords, no of words:\", len(clean_words))\n",
        "        # print(clean_words)\n",
        "        return clean_words\n",
        "\n",
        "    def tokenizing(self, parameters):\n",
        "        # Tokenising..............................................\n",
        "        data = '';\n",
        "        for i in parameters:\n",
        "            data += i + ' '\n",
        "        data = data.strip()\n",
        "        # print(data)\n",
        "        data = data.split(' ')\n",
        "        # print(data)\n",
        "        sentence = ''\n",
        "        for i in data:\n",
        "            sentence += i+' '\n",
        "        sentence=sentence.strip()\n",
        "        word_arr = word_tokenize(sentence.lower())\n",
        "        #  Here lower case is done to remove more stopwords, but some information is lost\n",
        "        # print( word_arr)\n",
        "        return word_arr\n",
        "\n",
        "class Node:\n",
        "    def __init__(self,entity,keywords):\n",
        "        self.entity=entity\n",
        "        # self.children=children\n",
        "        self.keywords=keywords\n",
        "        # self.deprecated=deprecated\n",
        "\n",
        "def ontology_creation():\n",
        "    with open('../input/ontobasedclassification/DOID_Ontology - Sheet1.csv', 'r') as csvFile:\n",
        "        reader = csv.reader(csvFile)\n",
        "        data = list(reader)\n",
        "    csvFile.close()\n",
        "    # for i in range(0,len(data[0])):\n",
        "    #   print(i,data[0][i])\n",
        "    # print(additional_keywords[0])\n",
        "    temp_object_for_keywords = keywords_from_ontology('')\n",
        "    nodes=[]\n",
        "    for k in range(1,len(data)):\n",
        "      parent_keywords=[]\n",
        "      child_keywords=[]\n",
        "      temp_object_for_keywords = keywords_from_ontology([str(data[k][0]), str(data[k][3]),\n",
        "                                                               str(data[k][4]),str(data[k][5]),\n",
        "                                                               str(data[k][6]), str(data[k][7]),\n",
        "                                                               str(data[k][7]),str(data[k][9]), \n",
        "                                                               str(data[k][10]),str(data[k][13]),\n",
        "                                                               str(data[k][14]),str(data[k][24]),\n",
        "                                                               str(data[k][25]),str(data[k][54]),\n",
        "                                                               str(data[k][55]),str(data[k][58])])      \n",
        "      for i in range(1,len(data)):\n",
        "        if data[i][0]==data[k][2]:\n",
        "          temp_object_for_keywords_parent = keywords_from_ontology([str(data[i][0]), str(data[i][3]),\n",
        "                                                               str(data[i][4]),str(data[i][5]),\n",
        "                                                               str(data[i][6]), str(data[i][7]),\n",
        "                                                               str(data[i][8]),str(data[i][9]), \n",
        "                                                               str(data[i][10]),str(data[i][13]),\n",
        "                                                               str(data[i][14]),str(data[i][24]),\n",
        "                                                               str(data[i][25]),str(data[i][54]),\n",
        "                                                               str(data[i][55]),str(data[i][58])])\n",
        "          parent_keywords=list(set(temp_object_for_keywords_parent.keywords_creation()))\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "      child_keywords=list(set(temp_object_for_keywords.keywords_creation()))\n",
        "      obj=Node(str(data[k][0]),list(set(child_keywords+parent_keywords)))\n",
        "      nodes.append(obj)\n",
        "    \n",
        "    return nodes\n",
        "\n",
        "def ontology_mathing(tree,keywords):\n",
        "    # print('Keywords formed from given x data:',keywords)\n",
        "    # print()\n",
        "    nodes_to_search=[]\n",
        "    found_nodes = []\n",
        "    priority_1 = []\n",
        "    priority_2 = []\n",
        "    priority_3 = []\n",
        "\n",
        "    for i in range(0, len(keywords)):\n",
        "        if keywords[i][1] != 1:\n",
        "            priority_1.append(keywords[i][0])\n",
        "        else:\n",
        "            if keywords[i][2] == 1:\n",
        "                priority_2.append(keywords[i][0])\n",
        "            else:\n",
        "                priority_3.append(keywords[i][0])\n",
        "\n",
        "    # nodes_to_search.append(tree[0])\n",
        "    # nodes_to_search.append(tree[76])\n",
        "    # i=0\n",
        "    # flag=0\n",
        "    # while i in range(0,len(nodes_to_search)):\n",
        "    #     if i==0:\n",
        "    #         node=nodes_to_search.index(tree[0])\n",
        "    #         print('Entity whose children are Currently Under Search:',tree[node].entity)\n",
        "    #         flag=0\n",
        "    #     else:\n",
        "    #         node = tree.index(nodes_to_search[i])\n",
        "    #         print('Entity whose children are Currently Under Search:', tree[node].entity)\n",
        "    #         flag=1\n",
        "    #     for j in range(0,len(tree[node].children)):\n",
        "    #         print('Child under Consideration:',tree[node].children[j])\n",
        "    #         for k in range(i,len(tree)):\n",
        "    #             if tree[k].entity==tree[node].children[j]:\n",
        "    #                 print('Found Entity in Tree:', tree[k].entity)\n",
        "    #                 if flag==0:\n",
        "    #                     for l in range(0,len(keywords)):\n",
        "    #                         if keywords[l][0] in tree[k].keywords:\n",
        "    #                             found_nodes.append((k,tree[k].entity,keywords[l][0]))\n",
        "    #                             nodes_to_search.append(tree[k])\n",
        "    #                             print('Keyword ',keywords[l][0],' Found in Node:',tree[k].entity)\n",
        "    #                             break\n",
        "    #                         else:\n",
        "    #                             continue\n",
        "    #                 else:\n",
        "    #                     nodes_to_search.append(tree[k])\n",
        "    #                     for l in range(0,len(keywords)):\n",
        "    #                         if keywords[l][0] in tree[k].keywords:\n",
        "    #                             found_nodes.append((k,tree[k].entity,keywords[l][0]))\n",
        "    #                             print('Keyword ',keywords[l][0],' Found in Node:',tree[k].entity)\n",
        "    #                             break\n",
        "    #                         else:\n",
        "    #                             continue\n",
        "    #             else:\n",
        "    #                 continue\n",
        "    #     i=i+1\n",
        "        # print(nodes_to_search)\n",
        "\n",
        "    for i in range(0,len(tree)):\n",
        "        priority_1_count = 0\n",
        "        priority_2_count = 0\n",
        "        priority_3_count = 0\n",
        "        for j in range(0,len(priority_1)):\n",
        "            if priority_1[j].lower() in [x.lower() for x in tree[i].keywords]:\n",
        "                priority_1_count = priority_1_count + 1\n",
        "            else:\n",
        "                continue\n",
        "        for j in range(0, len(priority_2)):\n",
        "            if priority_2[j].lower() in [x.lower() for x in tree[i].keywords]:\n",
        "                priority_2_count = priority_2_count + 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        for j in range(0, len(priority_3)):\n",
        "            if priority_3[j].lower() in [x.lower() for x in tree[i].keywords]:\n",
        "                priority_3_count = priority_3_count + 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        # print(\"Priority_1:\", priority_1_count)\n",
        "        # print(\"Priority_2:\", priority_2_count)\n",
        "        # print(\"Priority_3:\", priority_3_count)\n",
        "\n",
        "        # if priority_1_count>=1:\n",
        "        #     if priority_2_count>=1:\n",
        "        #         if priority_3_count>=1:\n",
        "        #             found_nodes.append((tree[i].entity))\n",
        "        #         else:\n",
        "        #             continue\n",
        "        #     else:\n",
        "        #         continue\n",
        "        # else:\n",
        "        #     continue\n",
        "        if priority_1_count+priority_2_count+priority_3_count>=1:\n",
        "            # print('Possible Node :', tree[i].entity)\n",
        "            # print('Possible Node Keywords:', tree[i].keywords)\n",
        "            # print('Keywords Passed:',keywords)\n",
        "            found_nodes.append((tree[i].entity))\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    # for i in range(0,len(found_nodes)):\n",
        "    #     print('Possible Class/Node:',found_nodes[i])\n",
        "    classes=[]\n",
        "    for i in range(0,len(found_nodes)):\n",
        "        classes.append(found_nodes[i])\n",
        "    classes=list(set(classes))\n",
        "    # print(classes)\n",
        "    return classes"
      ],
      "metadata": {
        "id": "S3DdZx-gTIkZ",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:39:39.120072Z",
          "iopub.execute_input": "2022-09-21T11:39:39.120453Z",
          "iopub.status.idle": "2022-09-21T11:39:39.154390Z",
          "shell.execute_reply.started": "2022-09-21T11:39:39.120423Z",
          "shell.execute_reply": "2022-09-21T11:39:39.153062Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Models Functions"
      ],
      "metadata": {
        "id": "uGPlZ9K6XUbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification without the Ontology"
      ],
      "metadata": {
        "id": "7hhIucPpMG_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------CLASSIFICATION PROCESS DEFINITION WITHOUT ONTOLOGY APPROACH---------\n",
        "def Classification_without_Ontology(x_train, y_train,x_test,y_test):\n",
        "    names=['Multinomial Naive Bayes','Bagging', 'KNN', 'SVM','Decision Tree','Random Forest']#,'Gradient Boost','Logistic Regression']\n",
        "    count_vector = CountVectorizer(max_features=400)\n",
        "    training_data = count_vector.fit_transform(x_train)\n",
        "    predictions=[]\n",
        "    testing_data = count_vector.transform(x_test)\n",
        " \n",
        "    naive_bayes = MultinomialNB()\n",
        "    grad_boost=GradientBoostingClassifier()\n",
        "    bagging=BaggingClassifier()\n",
        "    KNN=KNeighborsClassifier(n_neighbors=2)\n",
        "    SVM=LinearSVC()\n",
        "    decision_tree=DecisionTreeClassifier()\n",
        "    random_forest=RandomForestClassifier()\n",
        "    #lr=LogisticRegression()\n",
        "    \n",
        "    classifiers=[naive_bayes,bagging,KNN,SVM,decision_tree,random_forest]#,grad_boost,lr]\n",
        "    for i in range(0,len(classifiers)):\n",
        "      # DEBUG PRINTING\n",
        "      print('Classifier in use:'+names[i])\n",
        "      classifiers[i].fit(training_data, y_train)\n",
        "      # DEBUG PRINTING\n",
        "      print('Done with', names[i])\n",
        "    # print(count_vector.get_feature_names())\n",
        "      predictions.append(classifiers[i].predict(testing_data))\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "# ------CLASSIFICATION PROCESS EVALUATION WITHOUT ONTOLOGY APPROACH---------\n",
        "def entire_process_without_ontology():\n",
        "    data_x,data_y=dataset_creation()\n",
        "    names=['Multinomial Naive Bayes','Bagging', 'KNN', 'SVM','Decision Tree','Random Forest']#,'Gradient Boost']#,'Logistic Regression']\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2)\n",
        "    predictions_without_ontology = Classification_without_Ontology(x_train[:500], y_train[:500],x_test[:100],y_test[:100])\n",
        "    for i in range(0,len(predictions_without_ontology)):\n",
        "      print('Classifier under consideration: '+ names[i])\n",
        "      print('Accuracy score:',format(accuracy_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i])))\n",
        "      print('Precision score:',format(precision_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i], average='micro')))\n",
        "      print('Recall score:',format(recall_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i], average='micro')))\n",
        "      print('F1 score:',format(f1_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i], average='micro')))"
      ],
      "metadata": {
        "id": "xcsLhQo7ce-c",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:40:12.134835Z",
          "iopub.execute_input": "2022-09-21T11:40:12.135328Z",
          "iopub.status.idle": "2022-09-21T11:40:12.150638Z",
          "shell.execute_reply.started": "2022-09-21T11:40:12.135288Z",
          "shell.execute_reply": "2022-09-21T11:40:12.149601Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entire_process_without_ontology()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOpPxxYFch9c",
        "outputId": "4be1173b-c62f-42c1-f7de-13f85548a300",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:40:14.459165Z",
          "iopub.execute_input": "2022-09-21T11:40:14.459785Z",
          "iopub.status.idle": "2022-09-21T11:40:15.419126Z",
          "shell.execute_reply.started": "2022-09-21T11:40:14.459730Z",
          "shell.execute_reply": "2022-09-21T11:40:15.417950Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Classifier in use:Multinomial Naive Bayes\nDone with Multinomial Naive Bayes\nClassifier in use:Bagging\nDone with Bagging\nClassifier in use:KNN\nDone with KNN\nClassifier in use:SVM\nDone with SVM\nClassifier in use:Decision Tree\nDone with Decision Tree\nClassifier in use:Random Forest\nDone with Random Forest\nClassifier under consideration: Multinomial Naive Bayes\nAccuracy score: 0.49\nPrecision score: 0.49\nRecall score: 0.49\nF1 score: 0.49\nClassifier under consideration: Bagging\nAccuracy score: 0.33\nPrecision score: 0.33\nRecall score: 0.33\nF1 score: 0.33\nClassifier under consideration: KNN\nAccuracy score: 0.3\nPrecision score: 0.3\nRecall score: 0.3\nF1 score: 0.3\nClassifier under consideration: SVM\nAccuracy score: 0.6\nPrecision score: 0.6\nRecall score: 0.6\nF1 score: 0.6\nClassifier under consideration: Decision Tree\nAccuracy score: 0.29\nPrecision score: 0.29\nRecall score: 0.29\nF1 score: 0.29\nClassifier under consideration: Random Forest\nAccuracy score: 0.42\nPrecision score: 0.42\nRecall score: 0.42\nF1 score: 0.41999999999999993\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with the Ontology"
      ],
      "metadata": {
        "id": "AjT9S3G-LzV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------CLASSIFICATION PROCESS DEFINITION WITH ONTOLOGY APPROACH---------\n",
        "def Classification_with_Ontology(x_train, y_train,x_test,y_test):\n",
        "    ontology_tree=ontology_creation()\n",
        "    tree=ontology_tree\n",
        "    predictions=[[],[],[],[],[],[],[]]\n",
        "    c1,c2,c3=0,0,0\n",
        "    all_classes = list(set(list(set(y_train))+list(set(y_test))))\n",
        "    names=['Multinomial Naive Bayes','Bagging', 'KNN', 'SVM','Decision Tree','Random Forest','Logistic Regression']\n",
        "    keywords_for_matching=[]\n",
        "    \n",
        "    with open('../input/ontobasedclassification/Final_Dataset.csv', 'r') as csvFile:\n",
        "        reader = csv.reader(csvFile)\n",
        "        additional_keywords = list(reader)\n",
        "    csvFile.close()\n",
        "    \n",
        "    for i in range(1,len(additional_keywords)):\n",
        "      obj = cleaning_nlp_processes(additional_keywords[i][2]+additional_keywords[i][3])\n",
        "      additional_words = obj.keywords_formation()\n",
        "      # print('Keywords From Dataset for '+additional_keywords[i][0] +' are:')\n",
        "      # print(additional_words)\n",
        "      keywords_for_matching.append((additional_keywords[i][0],additional_words))\n",
        "\n",
        "    count_vector_entire_data_set = CountVectorizer(max_features=400)\n",
        "    training_data_entire_data_set = count_vector_entire_data_set.fit_transform(x_train)\n",
        "\n",
        "    naive_bayes_entire_data_set = MultinomialNB()\n",
        "    bagging_entire_data_set=BaggingClassifier()\n",
        "    KNN_entire_data_set=KNeighborsClassifier(n_neighbors=2)\n",
        "    SVM_entire_data_set=LinearSVC()\n",
        "    decision_tree_entire_data_set=DecisionTreeClassifier()\n",
        "    random_forest_entire_data_set=RandomForestClassifier()\n",
        "    lr_entire_dataset=LogisticRegression()\n",
        "    \n",
        "    classifiers_entire_data_set=[naive_bayes_entire_data_set,bagging_entire_data_set,KNN_entire_data_set,SVM_entire_data_set,decision_tree_entire_data_set,\n",
        "                                 random_forest_entire_data_set,lr_entire_dataset]\n",
        "    \n",
        "    naive_bayes = MultinomialNB()\n",
        "    bagging=BaggingClassifier()\n",
        "    KNN=KNeighborsClassifier(n_neighbors=2)\n",
        "    SVM=LinearSVC()\n",
        "    decision_tree=DecisionTreeClassifier()\n",
        "    random_forest=RandomForestClassifier()\n",
        "    lr=LogisticRegression()\n",
        "\n",
        "    classifiers=[naive_bayes,bagging,KNN,SVM,decision_tree,random_forest,lr]\n",
        "    \n",
        "    for i in range(0,len(classifiers_entire_data_set)):\n",
        "      # print('Classifier in use:'+names[i])\n",
        "      classifiers_entire_data_set[i].fit(training_data_entire_data_set, y_train)\n",
        "    # print(count_vector.get_feature_names())\n",
        "    \n",
        "    for z in range(0,len(x_test)):\n",
        "      # print('Disease Under Consideration:',y_test[z])\n",
        "      obj = cleaning_nlp_processes(x_test[z])\n",
        "      words = obj.keywords_formation()\n",
        "      # print('Initially Keywords from symtoms:')\n",
        "      # print(words)\n",
        "      for q in range(0,len(keywords_for_matching)):\n",
        "        # print(y_test[z],keywords_for_matching[q][0],y_test[z].strip()==keywords_for_matching[q][0].strip())\n",
        "        if y_test[z].strip()==keywords_for_matching[q][0].strip():\n",
        "          words=words+keywords_for_matching[q][1]\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "      # print('Adding additional Keywords From Dataset are:')\n",
        "      # print(words)\n",
        "      # flag=1\n",
        "      # for i in range(0,len(tree)):\n",
        "      #   if tree[i].entity[len(tree[i].entity)-1]==\"'\":\n",
        "      #     # print(tree[i].entity[0:len(tree[i].entity)-2].strip(),data_y[no])\n",
        "      #     if set(word_tokenize(y_test[z])).issubset(word_tokenize(tree[i].entity[0:len(tree[i].entity)-2])):\n",
        "      #       print('Ontology Node:',tree[i].entity[0:len(tree[i].entity)-2])\n",
        "      #       print('Keywords of ontology Node:',tree[i].keywords)\n",
        "      #       flag=0\n",
        "      #       break\n",
        "      #     else:\n",
        "      #       continue\n",
        "      #   else:      \n",
        "      #     # print(tree[i].entity,data_y[no])\n",
        "      #     if set(word_tokenize(y_test[z])).issubset(word_tokenize(tree[i].entity)):\n",
        "      #       print('Ontology Node:',tree[i].entity)\n",
        "      #       print('Keywords of ontology Node:',tree[i].keywords)\n",
        "      #       flag=0\n",
        "      #       break\n",
        "      #     else:\n",
        "      #       continue\n",
        "      common_classes=[]\n",
        "      # if flag==1:\n",
        "        # print('Disease doesn\"t occur in ontology')\n",
        "        # pass\n",
        "      possible_classes=ontology_mathing(ontology_tree,words)\n",
        "      # print('Possibilities After Ontology Matching:',possible_classes)\n",
        "      for i in range(0,len(all_classes)):\n",
        "          dataset_elements=[x.lower() for x in list(set(word_tokenize(all_classes[i])))]\n",
        "          for j in range(0,len(possible_classes)):\n",
        "            ontology_node=[x.lower() for x in list(set(word_tokenize(possible_classes[j])))]\n",
        "            if set(dataset_elements).issubset(set(ontology_node)):\n",
        "              common_classes.append(all_classes[i])\n",
        "            else:\n",
        "              continue      \n",
        "      common_classes=list(set(common_classes))\n",
        "      # print('Final Possibilites After Ontology Dataset Intersections:',common_classes)\n",
        "      # print('Number of possible Classes after intersection:',len(common_classes))\n",
        "      # print('Does Actual Class exsist in Possible Classes after intersection:', y_test[z] in common_classes)\n",
        "      if len(common_classes)==0:\n",
        "        c1=c1+1\n",
        "        # print('No Entry Found in Dataset and Ontology, Using Normal Classification.........')\n",
        "        # print('Comparing Predictions...............')\n",
        "        # print('Actual Value:',y_test[z])\n",
        "        testing_data = count_vector_entire_data_set.transform([x_test[z]])[0]\n",
        "        for h in range(0,len(classifiers_entire_data_set)):\n",
        "          prediction= classifiers_entire_data_set[h].predict(testing_data)[0] \n",
        "          predictions[h].append(prediction)\n",
        "          # print('Comparing Predictions...............')\n",
        "          # print('Classifier in use: ',names[h])\n",
        "          # print('Actual value: ',y_test[z])\n",
        "          # print('Predicted Value:',prediction)  \n",
        "      else:\n",
        "        c2=c2+1\n",
        "        # print('Using Ontology Matching............')\n",
        "        indexes_to_use = []\n",
        "        for j in range(0, len(y_train)):\n",
        "          if y_train[j] in common_classes:\n",
        "            indexes_to_use.append(j)\n",
        "          else:\n",
        "            continue\n",
        "        new_x=[x_train[x] for x in indexes_to_use] #qua sto solamente prendendo dal train solo le classi che ho trovato in comune con l'ontologia\n",
        "        new_y=[y_train[x] for x in indexes_to_use] # sia x che y\n",
        "\n",
        "        count_vector = CountVectorizer(max_features=400)\n",
        "        training_data = count_vector.fit_transform(new_x)\n",
        "        testing_data = count_vector.transform([x_test[z]])[0]\n",
        "        \n",
        "        for h in range(0,len(classifiers)):\n",
        "          classifiers[h].fit(training_data,new_y)\n",
        "          prediction= classifiers[h].predict(testing_data)[0] \n",
        "          predictions[h].append(prediction)\n",
        "          # print('Comparing Predictions...............')\n",
        "          # print('Classifier in use: ',names[h])\n",
        "          # print('Actual Value:',y_test[z])\n",
        "          # print('Predicted Value:',prediction)\n",
        "        # if set(word_tokenize(y_test[z])).issubset(set(word_tokenize(prediction))):\n",
        "          # c3=c3+1\n",
        "      # print(c1,c2,c3)\n",
        "      print(((z+1)/len(x_test))*100,'% Completed')\n",
        "      # print()\n",
        "    return predictions\n",
        "\n",
        "def entire_process_with_ontology():\n",
        "    names=['Multinomial Naive Bayes','Bagging', 'KNN', 'SVM','Decision Tree','Random Forest','Logistic Regression']#'Gradient Boost']\n",
        "    data_x,data_y=dataset_creation()\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2,random_state=0)\n",
        "    predictions_with_ontology = Classification_with_Ontology(x_train[:500],y_train[:500],x_test[:100] ,y_test[:100])\n",
        "    print(predictions_with_ontology)\n",
        "\n",
        "    for i in range(0,len(predictions_with_ontology)):\n",
        "      print('Classifier under consideration:'+ names[i])\n",
        "      print('Accuracy score:',format(accuracy_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i])))\n",
        "      print('Precision score:',format(precision_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro')))\n",
        "      print('Recall score:',format(recall_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro')))\n",
        "      print('F1 score:',format(f1_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro')))"
      ],
      "metadata": {
        "id": "HbP3bazRh9Qi",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:41:12.184509Z",
          "iopub.execute_input": "2022-09-21T11:41:12.184930Z",
          "iopub.status.idle": "2022-09-21T11:41:12.218331Z",
          "shell.execute_reply.started": "2022-09-21T11:41:12.184899Z",
          "shell.execute_reply": "2022-09-21T11:41:12.217258Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entire_process_with_ontology()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwSBFtaoXYw-",
        "outputId": "d02bfa22-50c7-4a90-9ccb-ea97d9fc6f66",
        "execution": {
          "iopub.status.busy": "2022-09-21T11:41:14.605912Z",
          "iopub.execute_input": "2022-09-21T11:41:14.606331Z",
          "iopub.status.idle": "2022-09-21T13:46:19.916252Z",
          "shell.execute_reply.started": "2022-09-21T11:41:14.606298Z",
          "shell.execute_reply": "2022-09-21T13:46:19.914986Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1.0 % Completed\n2.0 % Completed\n3.0 % Completed\n4.0 % Completed\n5.0 % Completed\n6.0 % Completed\n7.000000000000001 % Completed\n8.0 % Completed\n9.0 % Completed\n10.0 % Completed\n11.0 % Completed\n12.0 % Completed\n13.0 % Completed\n14.000000000000002 % Completed\n15.0 % Completed\n16.0 % Completed\n17.0 % Completed\n18.0 % Completed\n19.0 % Completed\n20.0 % Completed\n21.0 % Completed\n22.0 % Completed\n23.0 % Completed\n24.0 % Completed\n25.0 % Completed\n26.0 % Completed\n27.0 % Completed\n28.000000000000004 % Completed\n28.999999999999996 % Completed\n30.0 % Completed\n31.0 % Completed\n32.0 % Completed\n33.0 % Completed\n34.0 % Completed\n35.0 % Completed\n36.0 % Completed\n37.0 % Completed\n38.0 % Completed\n39.0 % Completed\n40.0 % Completed\n41.0 % Completed\n42.0 % Completed\n43.0 % Completed\n44.0 % Completed\n45.0 % Completed\n46.0 % Completed\n47.0 % Completed\n48.0 % Completed\n49.0 % Completed\n50.0 % Completed\n51.0 % Completed\n52.0 % Completed\n53.0 % Completed\n54.0 % Completed\n55.00000000000001 % Completed\n56.00000000000001 % Completed\n56.99999999999999 % Completed\n57.99999999999999 % Completed\n59.0 % Completed\n60.0 % Completed\n61.0 % Completed\n62.0 % Completed\n63.0 % Completed\n64.0 % Completed\n65.0 % Completed\n66.0 % Completed\n67.0 % Completed\n68.0 % Completed\n69.0 % Completed\n70.0 % Completed\n71.0 % Completed\n72.0 % Completed\n73.0 % Completed\n74.0 % Completed\n75.0 % Completed\n76.0 % Completed\n77.0 % Completed\n78.0 % Completed\n79.0 % Completed\n80.0 % Completed\n81.0 % Completed\n82.0 % Completed\n83.0 % Completed\n84.0 % Completed\n85.0 % Completed\n86.0 % Completed\n87.0 % Completed\n88.0 % Completed\n89.0 % Completed\n90.0 % Completed\n91.0 % Completed\n92.0 % Completed\n93.0 % Completed\n94.0 % Completed\n95.0 % Completed\n96.0 % Completed\n97.0 % Completed\n98.0 % Completed\n99.0 % Completed\n100.0 % Completed\n[['dyspnea', 'hemiparesis', 'hepatitis B', 'anxiety', 'cancer', 'arthritis', 'colon cancer', 'anxiety', 'Arthrogryposis', 'epilepsy', 'dependence', 'osteomyelitis', 'influenza', 'ischemia', 'myocardial infarction', 'diverticulitis', 'diverticulitis', 'Arthrogryposis', 'congestive heart failure', 'gastritis', 'pancytopenia', 'Arthrogryposis', 'cancer', 'influenza', 'diverticulitis', 'mitral valve insufficiency', 'Dysphagia', 'diverticulosis', 'respiratory failure', 'ischemia', 'neoplasm', 'neoplasm', 'adenocarcinoma', 'congestive heart failure', 'osteoporosis', 'hypoglycemia', 'gastritis', 'cancer', 'cancer', 'Arthrogryposis', 'dependence', 'urinary tract', 'psychotic disorder', 'cardiomyopathy', 'psychotic disorder', 'neoplasm', 'neoplasm', 'peptic ulcer', 'cancer', 'HIV', 'cellulitis', 'psychotic disorder', 'confusion', 'breast cancer', 'Arthrogryposis', 'diverticulitis', 'diverticulitis', 'neoplasm', 'respiratory failure', 'congestive heart failure', 'cancer', 'congestive heart failure', 'Dysphagia', 'jaundice', 'asthma', 'diabetes', 'peptic ulcer', 'endocarditis', 'pulmonary embolism', 'congestive heart failure', 'diverticulosis', 'cancer', 'Arthrogryposis', 'psychotic disorder', 'adenocarcinoma', 'psychotic disorder', 'incontinence', 'respiratory failure', 'Depression', 'Alcohol Dependence', 'ischemia', 'cardiomyopathy', 'HIV', 'asthma', 'arthritis', 'Aspiration Pneumonia', 'diabetic ketoacidosis', 'hemorrhoid', 'cancer', 'neoplasm', 'cirrhosis', 'jaundice', 'congestive heart failure', 'cirrhosis', 'hypothyroidism', 'jaundice', 'neoplasm', 'cancer', 'gastritis', 'HIV'], ['Pulmonary Hypertension', 'hemiparesis', 'hepatitis B', 'schizophrenia', 'cancer', 'arthritis', 'breast cancer', 'anxiety', 'Aspiration Pneumonia', 'Hemosiderosis', 'encephalopathy', 'osteomyelitis', 'influenza', 'hypertensive', 'myocardial infarction', 'tricuspid valve insufficiency', 'pancreatitis', 'Arthrogryposis', 'chronic obstructive pulmonary disease', 'aphasia', 'pancytopenia', 'Arthrogryposis', 'glaucoma', 'influenza', 'diverticulitis', 'tricuspid valve insufficiency', 'Dysphagia', 'melanoma', 'gout', 'pulmonary edema', 'neuropathy', 'Hepatocellular Carcinoma', 'pancreatitis', 'congestive heart failure', 'osteoporosis', 'hypoglycemia', 'encephalopathy', 'lymphatic system disease', 'jaundice', 'Arthrogryposis', 'dependence', 'decubitus ulcer', 'Depression', 'mitral valve insufficiency', 'depressive disorder', 'diabetic ketoacidosis', 'carcinoma of lung', 'cellulitis', 'neoplasm', 'decubitus ulcer', 'cellulitis', 'bipolar disorder', 'confusion', 'decubitus ulcer', 'endocarditis', 'pancytopenia', 'Cerebrovascular Disease', 'neoplasm', 'epilepsy', 'Pulmonary Hypertension', 'hepatitis', 'Pneumocystosis', 'Dysphagia', 'jaundice', 'asthma', 'hernia', 'peptic ulcer', 'Arthrogryposis', 'pulmonary embolism', 'cardiomyopathy', 'tricuspid valve insufficiency', 'hepatitis B', 'Arthrogryposis', 'psychotic disorder', 'gastritis', 'delusion', 'Cerebrovascular Disease', 'respiratory failure', 'psychotic disorder', 'Alcohol Dependence', 'hypercholesterolemia', 'Cerebrovascular Disease', 'asthma', 'asthma', 'arthritis', 'Thrombophlebitis', 'Cerebrovascular Disease', 'cirrhosis', 'endocarditis', 'breast cancer', 'cirrhosis', 'jaundice', 'chronic obstructive pulmonary disease', 'Arthrogryposis', 'tricuspid valve insufficiency', 'jaundice', 'Hemosiderosis', 'cirrhosis', 'gastritis', 'HIV'], ['dyspnea', 'Cerebrovascular Disease', 'hepatitis B', 'schizophrenia', 'cancer', 'arthritis', 'colon cancer', 'anxiety', 'Aspiration Pneumonia', 'Hemosiderosis', 'decubitus ulcer', 'gout', 'asthma', 'hypertensive', 'Pericardial Effusion', 'pancreatitis', 'gastritis', 'Arthrogryposis', 'Pulmonary Hypertension', 'Cerebrovascular Disease', 'Blood Coagulation', 'Arthrogryposis', 'ileus', 'Morbid Obesity', 'cholecystitis', 'mitral valve insufficiency', 'Dysphagia', 'diverticulosis', 'Cerebrovascular Disease', 'coronary heart disease', 'neuropathy', 'Hepatocellular Carcinoma', 'pancreatitis', 'decubitus ulcer', 'arthritis', 'hypoglycemia', 'aphasia', 'adenocarcinoma', 'cancer', 'Arthrogryposis', 'dependence', 'decubitus ulcer', 'epilepsy', 'cardiomyopathy', 'adhesion', 'kidney disease', 'carcinoma of lung', 'cellulitis', 'Morbid Obesity', 'decubitus ulcer', 'cellulitis', 'depressive disorder', 'confusion', 'decubitus ulcer', 'decubitus ulcer', 'Thrombocytopenia', 'cholelithiasis', 'neoplasm', 'epilepsy', 'Pulmonary Hypertension', 'asthma', 'Pulmonary Hypertension', 'Dysphagia', 'decubitus ulcer', 'asthma', 'migraine', 'decubitus ulcer', 'endocarditis', 'Cerebrovascular Disease', 'bronchitis', 'decubitus ulcer', 'hernia', 'Arthrogryposis', 'depressive disorder', 'gastritis', 'schizophrenia', 'Cerebrovascular Disease', 'respiratory failure', 'Depression', 'Alcohol Dependence', 'coronary heart disease', 'coronary heart disease', 'Pneumocystosis', 'asthma', 'arthritis', 'Thrombophlebitis', 'hepatitis B', 'hemorrhoid', 'endocarditis', 'endocarditis', 'cirrhosis', 'decubitus ulcer', 'Pulmonary Hypertension', 'arthritis', 'decubitus ulcer', 'jaundice', 'arthritis', 'neoplasm', 'gastritis', 'HIV'], ['dyspnea', 'hemiparesis', 'hepatitis B', 'anxiety', 'cancer', 'arthritis', 'colon cancer', 'anxiety', 'Aspiration Pneumonia', 'epilepsy', 'encephalopathy', 'osteomyelitis', 'influenza', 'hepatitis', 'myocardial infarction', 'colitis', 'pancreatitis', 'Arthrogryposis', 'chronic obstructive pulmonary disease', 'aphasia', 'pancytopenia', 'Arthrogryposis', 'ileus', 'chronic obstructive pulmonary disease', 'cholecystitis', 'mitral valve insufficiency', 'Dysphagia', 'diverticulosis', 'respiratory failure', 'hypertensive', 'neuropathy', 'tonic-clonic seizures', 'sinus tachycardia', 'melanoma', 'arthritis', 'hypoglycemia', 'gastritis', 'lymphatic system disease', 'cancer', 'Arthrogryposis', 'dependence', 'osteomyelitis', 'psychotic disorder', 'mitral valve insufficiency', 'manic disorder', 'kidney disease', 'carcinoma of lung', 'cellulitis', 'Morbid Obesity', 'gastroenteritis', 'cellulitis', 'psychotic disorder', 'confusion', 'breast cancer', 'Pulmonary Hypertension', 'Thrombocytopenia', 'Cerebrovascular Disease', 'neoplasm', 'respiratory failure', 'Pulmonary Hypertension', 'hepatitis', 'endocarditis', 'Dysphagia', 'jaundice', 'asthma', 'diabetes', 'peptic ulcer', 'endocarditis', 'Cerebrovascular Disease', 'congestive heart failure', 'ischemia', 'hernia', 'Arthrogryposis', 'psychotic disorder', 'adenocarcinoma', 'psychotic disorder', 'Cerebrovascular Disease', 'respiratory failure', 'Depression', 'Alcohol Dependence', 'coronary heart disease', 'dependence', 'Pneumocystosis', 'asthma', 'arthritis', 'Aspiration Pneumonia', 'diabetic ketoacidosis', 'hemorrhoid', 'endocarditis', 'endocarditis', 'cirrhosis', 'jaundice', 'congestive heart failure', 'hepatitis C', 'hypothyroidism', 'jaundice', 'cirrhosis', 'parkinson disease', 'asthma', 'HIV'], ['Pulmonary Hypertension', 'encephalopathy', 'hepatitis B', 'schizophrenia', 'jaundice', 'arthritis', 'breast cancer', 'bipolar disorder', 'tricuspid valve insufficiency', 'encephalopathy', 'encephalopathy', 'gout', 'influenza', 'hypertensive', 'myocardial infarction', 'pancreatitis', 'gastritis', 'Arthrogryposis', 'bronchitis', 'gout', 'tricuspid valve insufficiency', 'Pulmonary Hypertension', 'diabetic ketoacidosis', 'chronic obstructive pulmonary disease', 'cholecystitis', 'cardiomyopathy', 'hernia', 'melanoma', 'gout', 'coronary heart disease', 'neuropathy', 'metastasis', 'sinus tachycardia', 'congestive heart failure', 'osteoporosis', 'hypoglycemia', 'septicemia', 'lymphatic system disease', 'jaundice', 'Pulmonary Hypertension', 'dependence', 'urinary tract', 'delusion', 'cardiomyopathy', 'Depression', 'encephalopathy', 'breast cancer', 'infection', 'neoplasm', 'asthma', 'cellulitis', 'depressive disorder', 'Cerebrovascular Disease', 'decubitus ulcer', 'adhesion', 'pancytopenia', 'lymphoma', 'neoplasm', 'jaundice', 'Pulmonary Hypertension', 'Bronchial Disease', 'bronchitis', 'HIV', 'hepatitis C', 'bronchitis', 'parkinson disease', 'gout', 'neoplasm', 'tonic-clonic seizures', 'Pulmonary Hypertension', 'Pseudobulbar Palsy', 'hepatitis B', 'Pulmonary Hypertension', 'Depression', 'colon cancer', 'depressive disorder', 'Cerebrovascular Disease', 'gout', 'psychotic disorder', 'dependence', 'coronary heart disease', 'mitral valve insufficiency', 'Pseudobulbar Palsy', 'asthma', 'cellulitis', 'Aspiration Pneumonia', 'Cerebrovascular Disease', 'breast cancer', 'gastroenteritis', 'neoplasm', 'cirrhosis', 'Blood Coagulation', 'Pulmonary Hypertension', 'Arthrogryposis', 'Pseudobulbar Palsy', 'dependence', 'neoplasm', 'neutropenia', 'asthma', 'HIV'], ['dyspnea', 'hemiparesis', 'hepatitis B', 'schizophrenia', 'cancer', 'arthritis', 'lymphoma', 'anxiety', 'tricuspid valve insufficiency', 'epilepsy', 'encephalopathy', 'gout', 'influenza', 'hypertensive', 'hypercholesterolemia', 'pancreatitis', 'pancreatitis', 'Arthrogryposis', 'congestive heart failure', 'aphasia', 'pancytopenia', 'Arthrogryposis', 'ileus', 'Thrombocytopenia', 'cholecystitis', 'mitral valve insufficiency', 'Dysphagia', 'diverticulosis', 'ischemia', 'hypertensive', 'neuropathy', 'tonic-clonic seizures', 'sinus tachycardia', 'congestive heart failure', 'arthritis', 'hypoglycemia', 'hypertensive', 'lymphatic system disease', 'jaundice', 'Arthrogryposis', 'dependence', 'osteomyelitis', 'psychotic disorder', 'cardiomyopathy', 'manic disorder', 'migraine', 'carcinoma of lung', 'cellulitis', 'metastasis', 'decubitus ulcer', 'decubitus ulcer', 'psychotic disorder', 'confusion', 'decubitus ulcer', 'Pulmonary Hypertension', 'pancytopenia', 'melanoma', 'neoplasm', 'respiratory failure', 'Pulmonary Hypertension', 'asthma', 'chronic obstructive pulmonary disease', 'Dysphagia', 'jaundice', 'asthma', 'hypoglycemia', 'peptic ulcer', 'endocarditis', 'Cerebrovascular Disease', 'decubitus ulcer', 'tricuspid valve insufficiency', 'pancreatitis', 'Arthrogryposis', 'psychotic disorder', 'adenocarcinoma', 'psychotic disorder', 'Cerebrovascular Disease', 'respiratory failure', 'Depression', 'Alcohol Dependence', 'coronary heart disease', 'dependence', 'HIV', 'asthma', 'decubitus ulcer', 'Aspiration Pneumonia', 'hepatitis B', 'cirrhosis', 'endocarditis', 'neoplasm', 'cirrhosis', 'jaundice', 'Pulmonary Hypertension', 'hepatitis C', 'hypothyroidism', 'jaundice', 'arthritis', 'migraine', 'asthma', 'HIV'], ['dyspnea', 'hemiparesis', 'hepatitis B', 'anxiety', 'cancer', 'arthritis', 'colon cancer', 'anxiety', 'Arthrogryposis', 'epilepsy', 'encephalopathy', 'osteomyelitis', 'influenza', 'hypertensive', 'myocardial infarction', 'colitis', 'pancreatitis', 'Arthrogryposis', 'congestive heart failure', 'Cerebrovascular Disease', 'pancytopenia', 'Arthrogryposis', 'peptic ulcer', 'Morbid Obesity', 'diverticulitis', 'mitral valve insufficiency', 'Dysphagia', 'diverticulosis', 'respiratory failure', 'hypertensive', 'neuropathy', 'tonic-clonic seizures', 'sinus tachycardia', 'melanoma', 'osteoporosis', 'hypoglycemia', 'hypertensive', 'lymphatic system disease', 'cancer', 'Arthrogryposis', 'dependence', 'osteomyelitis', 'psychotic disorder', 'mitral valve insufficiency', 'manic disorder', 'encephalopathy', 'carcinoma of lung', 'cellulitis', 'metastasis', 'bronchitis', 'cellulitis', 'psychotic disorder', 'confusion', 'breast cancer', 'Pulmonary Hypertension', 'lymphoma', 'melanoma', 'neoplasm', 'respiratory failure', 'Pulmonary Hypertension', 'hepatitis', 'mitral valve insufficiency', 'Dysphagia', 'jaundice', 'asthma', 'urinary tract', 'peptic ulcer', 'endocarditis', 'Cerebrovascular Disease', 'congestive heart failure', 'tricuspid valve insufficiency', 'hepatitis B', 'Arthrogryposis', 'psychotic disorder', 'adenocarcinoma', 'psychotic disorder', 'Cerebrovascular Disease', 'respiratory failure', 'Depression', 'Alcohol Dependence', 'coronary heart disease', 'mitral valve insufficiency', 'Pneumocystosis', 'asthma', 'arthritis', 'Aspiration Pneumonia', 'diabetic ketoacidosis', 'hemorrhoid', 'endocarditis', 'endocarditis', 'cirrhosis', 'jaundice', 'congestive heart failure', 'hepatitis C', 'hypothyroidism', 'jaundice', 'metastasis', 'parkinson disease', 'gastritis', 'HIV']]\nClassifier under consideration:Multinomial Naive Bayes\nAccuracy score: 0.47\nPrecision score: 0.47\nRecall score: 0.47\nF1 score: 0.47\nClassifier under consideration:Bagging\nAccuracy score: 0.38\nPrecision score: 0.38\nRecall score: 0.38\nF1 score: 0.38\nClassifier under consideration:KNN\nAccuracy score: 0.41\nPrecision score: 0.41\nRecall score: 0.41\nF1 score: 0.41\nClassifier under consideration:SVM\nAccuracy score: 0.68\nPrecision score: 0.68\nRecall score: 0.68\nF1 score: 0.68\nClassifier under consideration:Decision Tree\nAccuracy score: 0.16\nPrecision score: 0.16\nRecall score: 0.16\nF1 score: 0.16\nClassifier under consideration:Random Forest\nAccuracy score: 0.52\nPrecision score: 0.52\nRecall score: 0.52\nF1 score: 0.52\nClassifier under consideration:Logistic Regression\nAccuracy score: 0.64\nPrecision score: 0.64\nRecall score: 0.64\nF1 score: 0.64\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Functions"
      ],
      "metadata": {
        "id": "eeIbgOUsYU46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Analysis Function"
      ],
      "metadata": {
        "id": "iR_mpcGcQ_LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_analysis():\n",
        "    total_count=0\n",
        "    c=0\n",
        "    print('Analysing Common Diseases in Dataset and Ontology.................')\n",
        "    #data_x,data_y=dataset_creation()\n",
        "    #ontology_tree = ontology_creation()\n",
        "    for i in list(set(data_y)):\n",
        "      print('Disease From Dataset:',i)\n",
        "      dataset_elements=[x.lower() for x in list(set(word_tokenize(i)))]\n",
        "      for j in range(0,len(ontology_tree)):\n",
        "            ontology_node=[x.lower() for x in list(set(word_tokenize(ontology_tree[j].entity)))]\n",
        "            if set(dataset_elements).issubset(set(ontology_node)):\n",
        "                c=c+1\n",
        "                # DEBUG PRINTING\n",
        "                print('Disease Name Ontology from ontology:', ontology_tree[j].entity)\n",
        "                # indices = [l for l, x in enumerate(data_y) if x == str(ontology_tree[i].entity)]\n",
        "                # print('Indices in Dataset for :',i, ' are:', indices)\n",
        "                obj = cleaning_nlp_processes(data_x[random.choice(indices)])\n",
        "                words = obj.keywords_formation()\n",
        "                print('Keywords From Dataset are:')\n",
        "                print(words)\n",
        "                obj = cleaning_nlp_processes(data_x[random.choice(indices)])\n",
        "                words = obj.keywords_formation()\n",
        "                print('Keywords From Dataset are:')\n",
        "                print(words)\n",
        "                obj = cleaning_nlp_processes(data_x[random.choice(indices)])\n",
        "                words = obj.keywords_formation()\n",
        "                print('Keywords From Dataset are:')\n",
        "                print(words)\n",
        "                obj = cleaning_nlp_processes(data_x[random.choice(indices)])\n",
        "                words = obj.keywords_formation()\n",
        "                print('Keywords From Dataset are:')\n",
        "                print(words)\n",
        "                print('Ontology Information:')\n",
        "                print('Keywords From Ontology Node:')\n",
        "                print(ontology_tree[j].keywords)\n",
        "                print('Children of Ontology Node:',ontology_tree[i].children)\n",
        "                # END DEBUG PHASE\n",
        "            else:\n",
        "                continue\n",
        "      print('Number of Nodes Match for this disease are:',c)\n",
        "      if c!=0:\n",
        "        total_count=total_count+1\n",
        "      else:\n",
        "        print(\"This Disease can't matched in the ontology:\",i)\n",
        "      c=0\n",
        "    print(total_count)"
      ],
      "metadata": {
        "id": "JaOO1qDwRBoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_analysis()"
      ],
      "metadata": {
        "id": "wX58YDYnRD_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the notebook in PDF"
      ],
      "metadata": {
        "id": "uMPqtOQCpjxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\n",
        "\n",
        "import re, pathlib, shutil\n",
        "\n",
        "# Get a list of all your Notebooks\n",
        "notebooks = [x for x in pathlib.Path(\"/content/drive/My Drive/Colab Notebooks\").iterdir() if \n",
        "             re.search(r\"\\.ipynb\", x.name, flags = re.I)]\n",
        "\n",
        "for i, n in enumerate(notebooks):\n",
        "    print(f\"\\nProcessing  [{i+1:{len(str(len(notebooks)))}d}/{len(notebooks)}]  {n.name}\\n\")\n",
        "\n",
        "    # Optionally copy your notebooks from gdrive to your vm\n",
        "    shutil.copy(n, n.name)\n",
        "    n = pathlib.Path(n.name)\n",
        "\n",
        "!jupyter nbconvert \"{n.as_posix()}\" --to pdf --output \"{n.stem.replace(\" \", \"_\")}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkZ1hjl_xABH",
        "outputId": "020cae48-62b3-47cb-9623-036715773315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Connecting to security.ubuntu.com (185.125.190.39)] [Connected to cloud.r-p\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\u001b[0m\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\u001b[0m\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\u001b[0m\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\u001b[0m\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r                                                                               \r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                        \rHit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                        \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "texlive-fonts-recommended is already the newest version (2017.20180305-1).\n",
            "texlive-generic-recommended is already the newest version (2017.20180305-1).\n",
            "texlive-xetex is already the newest version (2017.20180305-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "\n",
            "Processing  [ 1/35]  DataLoader.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 2/35]  CCS-AlexNet.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 3/35]  mini-contest-3.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 4/35]  Template FuseMed.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 5/35]  Info_Ret_testo.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 6/35]  Homework3_BDE.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 7/35]  Untitled0.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 8/35]  Analysis.ipynb\n",
            "\n",
            "\n",
            "Processing  [ 9/35]  homework3-bde (1).ipynb\n",
            "\n",
            "\n",
            "Processing  [10/35]  Hackaton_KPNG.ipynb\n",
            "\n",
            "\n",
            "Processing  [11/35]  Hackaton_KPNG (1).ipynb\n",
            "\n",
            "\n",
            "Processing  [12/35]  Elaborato_CCS.ipynb\n",
            "\n",
            "\n",
            "Processing  [13/35]  1_1_First_steps_with_MongoDB_PyMongo.ipynb\n",
            "\n",
            "\n",
            "Processing  [14/35]  Untitled6.ipynb\n",
            "\n",
            "\n",
            "Processing  [15/35]  WorkshopPySpark_English_NoSolution.ipynb\n",
            "\n",
            "\n",
            "Processing  [16/35]  data_cleaning_and_merging.ipynb\n",
            "\n",
            "\n",
            "Processing  [17/35]  Untitled6 (6).ipynb\n",
            "\n",
            "\n",
            "Processing  [18/35]  PreProcessing_and_Merging.ipynb\n",
            "\n",
            "\n",
            "Processing  [19/35]  Untitled6 (7).ipynb\n",
            "\n",
            "\n",
            "Processing  [20/35]  Untitled1.ipynb\n",
            "\n",
            "\n",
            "Processing  [21/35]  KPMG_hack.ipynb\n",
            "\n",
            "\n",
            "Processing  [22/35]  DataStorage.ipynb\n",
            "\n",
            "\n",
            "Processing  [23/35]  Untitled6 (10).ipynb\n",
            "\n",
            "\n",
            "Processing  [24/35]  DataAnalysisAndAnalysisStorage.ipynb\n",
            "\n",
            "\n",
            "Processing  [25/35]  Mini_Contest_3.ipynb\n",
            "\n",
            "\n",
            "Processing  [26/35]  bird-classification_completo.ipynb\n",
            "\n",
            "\n",
            "Processing  [27/35]  bird-classification.ipynb\n",
            "\n",
            "\n",
            "Processing  [28/35]  Information-Retrieval.ipynb\n",
            "\n",
            "\n",
            "Processing  [29/35]  information-retrieval-2.ipynb\n",
            "\n",
            "\n",
            "Processing  [30/35]  Ontology-classification.ipynb\n",
            "\n",
            "\n",
            "Processing  [31/35]  Text-Preprocessing.ipynb\n",
            "\n",
            "\n",
            "Processing  [32/35]  information-retrieval3.ipynb\n",
            "\n",
            "\n",
            "Processing  [33/35]  Ontology-based-classification.ipynb\n",
            "\n",
            "\n",
            "Processing  [34/35]  Ontology-based-classification-2.ipynb\n",
            "\n",
            "\n",
            "Processing  [35/35]  onto-classification-kaggle.ipynb\n",
            "\n",
            "[NbConvertApp] Converting notebook onto-classification-kaggle.ipynb to pdf\n",
            "[NbConvertApp] Writing 156815 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 117321 bytes to onto-classification-kaggle.pdf\n"
          ]
        }
      ]
    }
  ]
}